{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import atexit\n",
    "import datetime\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import tensorboardX\n",
    "\n",
    "from prob_mbrl import utils, models, algorithms, envs\n",
    "from functools import partial\n",
    "torch.set_flush_denormal(True)\n",
    "torch.set_num_threads(1)\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DynamicsModel(\n",
      "  (model): BSequential(\n",
      "    (fc0): Linear(in_features=6, out_features=200, bias=True)\n",
      "    (nonlin0): ReLU()\n",
      "    (drop0): CDropout(rate=0.25, temperature=0.10000000149011612, regularizer_scale=0.5)\n",
      "    (fc1): Linear(in_features=200, out_features=200, bias=True)\n",
      "    (nonlin1): ReLU()\n",
      "    (drop1): CDropout(rate=0.25, temperature=0.10000000149011612, regularizer_scale=0.5)\n",
      "    (fc_out): Linear(in_features=200, out_features=10, bias=True)\n",
      "  )\n",
      "  (output_density): DiagGaussianDensity(output_dims=5)\n",
      "  (reward_func): CartpoleReward()\n",
      ")\n",
      "Policy(\n",
      "  (model): BSequential(\n",
      "    (fc0): Linear(in_features=5, out_features=200, bias=True)\n",
      "    (nonlin0): ReLU()\n",
      "    (drop0): BDropout(rate=0.10000000149011612, regularizer_scale=0.5)\n",
      "    (fc1): Linear(in_features=200, out_features=200, bias=True)\n",
      "    (nonlin1): ReLU()\n",
      "    (drop1): BDropout(rate=0.10000000149011612, regularizer_scale=0.5)\n",
      "    (fc_out): Linear(in_features=200, out_features=2, bias=True)\n",
      "    (fc_nonlin): DiagGaussianDensity(output_dims=1)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "n_rnd = 4\n",
    "pred_H = 25\n",
    "control_H = 40\n",
    "N_particles = 100\n",
    "pol_opt_iters = 1000\n",
    "dyn_opt_iters = 2000\n",
    "ps_iters = 100\n",
    "dyn_components = 1\n",
    "dyn_shape = [200] * 2\n",
    "pol_shape = [200] * 2\n",
    "use_cuda = False\n",
    "learn_reward = False\n",
    "keep_best = False\n",
    "\n",
    "# initialize environment\n",
    "# env = envs.Pendulum() # this works better with learning the reward function\n",
    "env = envs.Cartpole()\n",
    "\n",
    "results_filename = os.path.expanduser(\n",
    "    \"~/.prob_mbrl/results_%s_%s.pth.tar\" %\n",
    "    (env.__class__.__name__,\n",
    "     datetime.datetime.now().strftime(\"%Y%m%d%H%M%S.%f\")))\n",
    "D = env.observation_space.shape[0]\n",
    "U = env.action_space.shape[0]\n",
    "maxU = env.action_space.high\n",
    "minU = env.action_space.low\n",
    "\n",
    "# initialize reward/cost function\n",
    "if learn_reward or env.reward_func is None:\n",
    "    reward_func = None\n",
    "else:\n",
    "    reward_func = env.reward_func\n",
    "\n",
    "# intialize to max episode steps if available\n",
    "if hasattr(env, 'spec'):\n",
    "    if hasattr(env.spec, 'max_episode_steps'):\n",
    "        control_H = env.spec.max_episode_steps\n",
    "initial_experience = control_H * n_rnd\n",
    "\n",
    "# initialize dynamics model\n",
    "dynE = 2 * (D + 1) if learn_reward else 2 * D\n",
    "if dyn_components > 1:\n",
    "    output_density = models.GaussianMixtureDensity(dynE / 2,\n",
    "                                                   dyn_components)\n",
    "    dynE = (dynE + 1) * dyn_components + 1\n",
    "else:\n",
    "    output_density = models.DiagGaussianDensity(dynE / 2)\n",
    "\n",
    "dyn_model = models.mlp(D + U,\n",
    "                       dynE,\n",
    "                       dyn_shape,\n",
    "                       dropout_layers=[\n",
    "                           models.modules.CDropout(0.25, 0.1)\n",
    "                           for i in range(len(dyn_shape))\n",
    "                       ],\n",
    "                       nonlin=torch.nn.ReLU)\n",
    "dyn = models.DynamicsModel(dyn_model,\n",
    "                           reward_func=reward_func,\n",
    "                           output_density=output_density).float()\n",
    "\n",
    "# initalize policy\n",
    "pol_model = models.mlp(D,\n",
    "                       2 * U,\n",
    "                       pol_shape,\n",
    "                       dropout_layers=[\n",
    "                           models.modules.BDropout(0.1)\n",
    "                           for i in range(len(pol_shape))\n",
    "                       ],\n",
    "                       nonlin=torch.nn.ReLU,\n",
    "                       output_nonlin=partial(models.DiagGaussianDensity,\n",
    "                                             U))\n",
    "\n",
    "pol = models.Policy(pol_model, maxU, minU).float()\n",
    "print(dyn)\n",
    "print(pol)\n",
    "\n",
    "# initalize experience dataset\n",
    "exp = utils.ExperienceDataset()\n",
    "\n",
    "# initialize dynamics optimizer\n",
    "opt1 = torch.optim.Adam(dyn.parameters(), 1e-4)\n",
    "\n",
    "# initialize policy optimizer\n",
    "opt2 = torch.optim.Adam(pol.parameters(), 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply_controller Starting run\n",
      "apply_controller Running for 4.000000 seconds\n",
      "apply_controller Done after [40] steps. Stopping robot. Value of run [0.009175]\n",
      "apply_controller Starting run\n",
      "apply_controller Running for 4.000000 seconds\n",
      "apply_controller Done after [40] steps. Stopping robot. Value of run [0.572404]\n",
      "apply_controller Starting run\n",
      "apply_controller Running for 4.000000 seconds\n",
      "apply_controller Done after [40] steps. Stopping robot. Value of run [0.006426]\n",
      "apply_controller Starting run\n",
      "apply_controller Running for 4.000000 seconds\n",
      "apply_controller Done after [40] steps. Stopping robot. Value of run [0.025665]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log-likelihood of data: -7.943673:   1%|          | 14/2000 [00:00<00:14, 135.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_regressor > Dataset size [156]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log-likelihood of data: -0.029115: 100%|█████████▉| 1996/2000 [00:12<00:00, 170.74it/s]\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy search iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pred. Cumm. rewards: 3.800731 [25]: 100%|██████████| 1000/1000 [01:51<00:00,  9.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply_controller Starting run\n",
      "apply_controller Running for 4.000000 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log-likelihood of data: -0.215226:   1%|          | 16/2000 [00:00<00:13, 151.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply_controller Done after [40] steps. Stopping robot. Value of run [1.958003]\n",
      "train_regressor > Dataset size [195]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log-likelihood of data: 1.654886: 100%|█████████▉| 1998/2000 [00:12<00:00, 157.80it/s]\n",
      "Pred. Cumm. rewards: 1.067301 [25]:   0%|          | 1/1000 [00:00<01:59,  8.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy search iteration 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pred. Cumm. rewards: 7.496438 [25]: 100%|██████████| 1000/1000 [01:43<00:00,  9.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply_controller Starting run\n",
      "apply_controller Running for 4.000000 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log-likelihood of data: 0.920200:   1%|          | 17/2000 [00:00<00:12, 162.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply_controller Done after [40] steps. Stopping robot. Value of run [11.374109]\n",
      "train_regressor > Dataset size [234]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log-likelihood of data: 2.813802:  99%|█████████▉| 1986/2000 [00:12<00:00, 163.58it/s]\n",
      "Pred. Cumm. rewards: 3.125003 [25]:   0%|          | 1/1000 [00:00<01:43,  9.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy search iteration 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pred. Cumm. rewards: 7.434040 [25]: 100%|██████████| 1000/1000 [01:44<00:00,  9.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply_controller Starting run\n",
      "apply_controller Running for 4.000000 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log-likelihood of data: 2.049290:   1%|          | 16/2000 [00:00<00:12, 159.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply_controller Done after [40] steps. Stopping robot. Value of run [10.344432]\n",
      "train_regressor > Dataset size [273]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log-likelihood of data: 2.775593: 100%|█████████▉| 1997/2000 [00:12<00:00, 156.08it/s]\n",
      "Pred. Cumm. rewards: 4.749218 [25]:   0%|          | 1/1000 [00:00<01:43,  9.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy search iteration 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pred. Cumm. rewards: 6.825605 [25]: 100%|██████████| 1000/1000 [01:45<00:00,  9.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply_controller Starting run\n",
      "apply_controller Running for 4.000000 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log-likelihood of data: 2.678162:   1%|          | 16/2000 [00:00<00:12, 154.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply_controller Done after [40] steps. Stopping robot. Value of run [5.977985]\n",
      "train_regressor > Dataset size [312]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log-likelihood of data: 3.355496: 100%|█████████▉| 1996/2000 [00:12<00:00, 162.39it/s]\n",
      "Pred. Cumm. rewards: 4.069183 [25]:   0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy search iteration 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pred. Cumm. rewards: 6.353509 [25]: 100%|██████████| 1000/1000 [01:43<00:00,  9.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply_controller Starting run\n",
      "apply_controller Running for 4.000000 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log-likelihood of data: 3.251989:   1%|          | 16/2000 [00:00<00:12, 156.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply_controller Done after [40] steps. Stopping robot. Value of run [9.977594]\n",
      "train_regressor > Dataset size [351]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log-likelihood of data: 4.562339: 100%|█████████▉| 1998/2000 [00:12<00:00, 160.43it/s]\n",
      "Pred. Cumm. rewards: 4.943288 [25]:   0%|          | 1/1000 [00:00<01:42,  9.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy search iteration 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pred. Cumm. rewards: 6.858777 [25]: 100%|██████████| 1000/1000 [01:45<00:00,  9.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply_controller Starting run\n",
      "apply_controller Running for 4.000000 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log-likelihood of data: 3.870353:   1%|          | 16/2000 [00:00<00:13, 149.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply_controller Done after [40] steps. Stopping robot. Value of run [10.828394]\n",
      "train_regressor > Dataset size [390]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log-likelihood of data: 5.380519: 100%|█████████▉| 1999/2000 [00:13<00:00, 150.43it/s]\n",
      "Pred. Cumm. rewards: 4.559573 [25]:   0%|          | 1/1000 [00:00<01:48,  9.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy search iteration 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pred. Cumm. rewards: 6.050838 [25]: 100%|██████████| 1000/1000 [01:45<00:00,  9.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply_controller Starting run\n",
      "apply_controller Running for 4.000000 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log-likelihood of data: 5.490703:   1%|          | 15/2000 [00:00<00:13, 147.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply_controller Done after [40] steps. Stopping robot. Value of run [9.875975]\n",
      "train_regressor > Dataset size [429]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log-likelihood of data: 5.804075: 100%|█████████▉| 1998/2000 [00:12<00:00, 149.93it/s]\n",
      "Pred. Cumm. rewards: 5.038359 [25]:   0%|          | 1/1000 [00:00<01:45,  9.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy search iteration 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pred. Cumm. rewards: 6.614202 [25]: 100%|██████████| 1000/1000 [01:44<00:00,  9.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply_controller Starting run\n",
      "apply_controller Running for 4.000000 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log-likelihood of data: 5.542390:   1%|          | 15/2000 [00:00<00:13, 141.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply_controller Done after [40] steps. Stopping robot. Value of run [9.766203]\n",
      "train_regressor > Dataset size [468]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log-likelihood of data: 6.396270:  99%|█████████▉| 1986/2000 [00:12<00:00, 160.32it/s]\n",
      "Pred. Cumm. rewards: 5.009095 [25]:   0%|          | 2/1000 [00:00<01:35, 10.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy search iteration 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pred. Cumm. rewards: 8.403789 [25]: 100%|██████████| 1000/1000 [01:44<00:00,  9.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply_controller Starting run\n",
      "apply_controller Running for 4.000000 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log-likelihood of data: 6.405999:   1%|          | 16/2000 [00:00<00:12, 156.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply_controller Done after [40] steps. Stopping robot. Value of run [6.155827]\n",
      "train_regressor > Dataset size [507]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log-likelihood of data: 6.576864: 100%|█████████▉| 1991/2000 [00:12<00:00, 159.05it/s]\n",
      "Pred. Cumm. rewards: 4.414504 [25]:   0%|          | 1/1000 [00:00<01:46,  9.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy search iteration 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pred. Cumm. rewards: 9.068583 [25]: 100%|██████████| 1000/1000 [01:45<00:00,  9.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply_controller Starting run\n",
      "apply_controller Running for 4.000000 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log-likelihood of data: 6.374048:   1%|          | 17/2000 [00:00<00:11, 165.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply_controller Done after [40] steps. Stopping robot. Value of run [18.865135]\n",
      "train_regressor > Dataset size [546]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log-likelihood of data: 7.658630: 100%|█████████▉| 1995/2000 [00:12<00:00, 168.03it/s]\n",
      "Pred. Cumm. rewards: 6.258204 [25]:   0%|          | 2/1000 [00:00<01:37, 10.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy search iteration 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pred. Cumm. rewards: 16.094427 [25]: 100%|██████████| 1000/1000 [01:44<00:00,  9.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply_controller Starting run\n",
      "apply_controller Running for 4.000000 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log-likelihood of data: 6.727750:   1%|          | 16/2000 [00:00<00:12, 157.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply_controller Done after [40] steps. Stopping robot. Value of run [5.804806]\n",
      "train_regressor > Dataset size [585]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log-likelihood of data: 8.120251:  99%|█████████▉| 1984/2000 [00:12<00:00, 154.57it/s]\n",
      "Pred. Cumm. rewards: 4.405800 [25]:   0%|          | 2/1000 [00:00<01:36, 10.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy search iteration 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pred. Cumm. rewards: 16.438950 [25]: 100%|██████████| 1000/1000 [01:44<00:00,  9.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply_controller Starting run\n",
      "apply_controller Running for 4.000000 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log-likelihood of data: 7.035192:   0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply_controller Done after [40] steps. Stopping robot. Value of run [30.615314]\n",
      "train_regressor > Dataset size [624]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log-likelihood of data: 8.469097: 100%|█████████▉| 1996/2000 [00:12<00:00, 160.32it/s]\n",
      "Pred. Cumm. rewards: 4.171557 [25]:   0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy search iteration 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pred. Cumm. rewards: 17.038742 [25]: 100%|██████████| 1000/1000 [01:44<00:00,  9.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply_controller Starting run\n",
      "apply_controller Running for 4.000000 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log-likelihood of data: 7.769278:   0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply_controller Done after [40] steps. Stopping robot. Value of run [31.781195]\n",
      "train_regressor > Dataset size [663]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log-likelihood of data: 9.246945: 100%|█████████▉| 1992/2000 [00:12<00:00, 158.02it/s]\n",
      "Pred. Cumm. rewards: 4.866514 [25]:   0%|          | 1/1000 [00:00<01:51,  8.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy search iteration 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pred. Cumm. rewards: 17.507059 [25]: 100%|██████████| 1000/1000 [01:44<00:00,  9.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply_controller Starting run\n",
      "apply_controller Running for 4.000000 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log-likelihood of data: 8.057062:   1%|          | 17/2000 [00:00<00:12, 163.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply_controller Done after [40] steps. Stopping robot. Value of run [32.368610]\n",
      "train_regressor > Dataset size [702]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log-likelihood of data: 9.202250: 100%|█████████▉| 1997/2000 [00:12<00:00, 156.43it/s] \n",
      "Pred. Cumm. rewards: 3.817450 [25]:   0%|          | 1/1000 [00:00<01:41,  9.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy search iteration 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pred. Cumm. rewards: 17.507593 [25]: 100%|██████████| 1000/1000 [01:43<00:00,  9.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply_controller Starting run\n",
      "apply_controller Running for 4.000000 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log-likelihood of data: 9.539936:   1%|          | 16/2000 [00:00<00:12, 159.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply_controller Done after [40] steps. Stopping robot. Value of run [31.940863]\n",
      "train_regressor > Dataset size [741]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log-likelihood of data: 9.680073: 100%|█████████▉| 1997/2000 [00:12<00:00, 156.34it/s] \n",
      "Pred. Cumm. rewards: 5.758907 [25]:   0%|          | 1/1000 [00:00<01:46,  9.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy search iteration 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pred. Cumm. rewards: 17.689659 [25]: 100%|██████████| 1000/1000 [01:44<00:00,  9.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply_controller Starting run\n",
      "apply_controller Running for 4.000000 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log-likelihood of data: 9.633832:   1%|          | 15/2000 [00:00<00:13, 142.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply_controller Done after [40] steps. Stopping robot. Value of run [32.487495]\n",
      "train_regressor > Dataset size [780]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log-likelihood of data: 10.581008: 100%|█████████▉| 1994/2000 [00:12<00:00, 153.55it/s]\n",
      "Pred. Cumm. rewards: 2.599709 [25]:   0%|          | 1/1000 [00:00<01:53,  8.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy search iteration 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pred. Cumm. rewards: 17.726728 [25]: 100%|██████████| 1000/1000 [01:44<00:00,  9.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply_controller Starting run\n",
      "apply_controller Running for 4.000000 seconds\n",
      "apply_controller Done after [40] steps. Stopping robot. Value of run [32.873203]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log-likelihood of data: 10.220001:   1%|          | 16/2000 [00:00<00:13, 151.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_regressor > Dataset size [819]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log-likelihood of data: 10.972873:  99%|█████████▉| 1985/2000 [00:12<00:00, 156.80it/s]\n",
      "Pred. Cumm. rewards: 6.075938 [25]:   0%|          | 1/1000 [00:00<01:48,  9.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy search iteration 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pred. Cumm. rewards: 17.802050 [25]: 100%|██████████| 1000/1000 [01:46<00:00,  9.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply_controller Starting run\n",
      "apply_controller Running for 4.000000 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log-likelihood of data: 10.808597:   0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply_controller Done after [40] steps. Stopping robot. Value of run [33.014568]\n",
      "train_regressor > Dataset size [858]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log-likelihood of data: 12.225194:  99%|█████████▉| 1987/2000 [00:11<00:00, 173.79it/s]\n",
      "Pred. Cumm. rewards: 4.181459 [25]:   0%|          | 1/1000 [00:00<01:49,  9.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy search iteration 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pred. Cumm. rewards: 17.770880 [25]: 100%|██████████| 1000/1000 [01:39<00:00,  9.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply_controller Starting run\n",
      "apply_controller Running for 4.000000 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log-likelihood of data: 12.590409:   0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply_controller Done after [40] steps. Stopping robot. Value of run [32.674847]\n",
      "train_regressor > Dataset size [897]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log-likelihood of data: 12.773203:  99%|█████████▉| 1988/2000 [00:12<00:00, 159.45it/s]\n",
      "Pred. Cumm. rewards: 4.142340 [25]:   0%|          | 1/1000 [00:00<01:51,  8.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy search iteration 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pred. Cumm. rewards: 17.944530 [25]: 100%|██████████| 1000/1000 [01:41<00:00, 10.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply_controller Starting run\n",
      "apply_controller Running for 4.000000 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log-likelihood of data: 11.522026:   0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply_controller Done after [40] steps. Stopping robot. Value of run [33.002983]\n",
      "train_regressor > Dataset size [936]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log-likelihood of data: 13.655376:  99%|█████████▉| 1989/2000 [00:12<00:00, 160.44it/s]\n",
      "Pred. Cumm. rewards: 3.662322 [25]:   0%|          | 1/1000 [00:00<01:50,  9.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy search iteration 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pred. Cumm. rewards: 18.035997 [25]: 100%|██████████| 1000/1000 [01:38<00:00, 10.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply_controller Starting run\n",
      "apply_controller Running for 4.000000 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log-likelihood of data: 12.243679:   0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply_controller Done after [40] steps. Stopping robot. Value of run [32.837471]\n",
      "train_regressor > Dataset size [975]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log-likelihood of data: 13.260921: 100%|█████████▉| 1992/2000 [00:12<00:00, 160.97it/s]\n",
      "Pred. Cumm. rewards: 3.975950 [25]:   0%|          | 1/1000 [00:00<01:56,  8.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy search iteration 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pred. Cumm. rewards: 17.972471 [25]: 100%|██████████| 1000/1000 [01:41<00:00, 11.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply_controller Starting run\n",
      "apply_controller Running for 4.000000 seconds\n",
      "apply_controller Done after [40] steps. Stopping robot. Value of run [33.013786]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log-likelihood of data: 12.329764:   1%|          | 14/2000 [00:00<00:14, 138.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_regressor > Dataset size [1014]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log-likelihood of data: 14.467130:  99%|█████████▉| 1988/2000 [00:14<00:00, 134.70it/s]\n",
      "Pred. Cumm. rewards: 11.993865 [25]:   0%|          | 2/1000 [00:00<01:34, 10.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy search iteration 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pred. Cumm. rewards: 18.055079 [25]: 100%|██████████| 1000/1000 [01:30<00:00, 11.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply_controller Starting run\n",
      "apply_controller Running for 4.000000 seconds\n",
      "apply_controller Done after [40] steps. Stopping robot. Value of run [32.969631]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log-likelihood of data: 13.322544:   1%|          | 14/2000 [00:00<00:15, 131.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_regressor > Dataset size [1053]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "log-likelihood of data: 13.291028:  56%|█████▋    | 1129/2000 [00:08<00:06, 132.01it/s]"
     ]
    }
   ],
   "source": [
    "%matplotlib qt5\n",
    "if use_cuda and torch.cuda.is_available():\n",
    "    dyn = dyn.cuda()\n",
    "    pol = pol.cuda()\n",
    "\n",
    "writer = tensorboardX.SummaryWriter()\n",
    "\n",
    "# callbacks\n",
    "def on_close():\n",
    "    writer.close()\n",
    "\n",
    "atexit.register(on_close)\n",
    "\n",
    "# initial experience data collection\n",
    "scale = maxU - minU\n",
    "bias = minU\n",
    "rnd = lambda x, t: (scale * np.random.rand(U, ) + bias)  # noqa: E731\n",
    "while exp.n_samples() < initial_experience:\n",
    "    ret = utils.apply_controller(\n",
    "        env,\n",
    "        rnd,\n",
    "        control_H,\n",
    "        realtime=False,\n",
    "        stop_when_done=False,\n",
    "        callback=lambda *args, **kwargs: env.render())\n",
    "    params_ = [p.clone() for p in list(pol.parameters())]\n",
    "    exp.append_episode(*ret, policy_params=params_)\n",
    "    exp.save(results_filename)\n",
    "\n",
    "# policy learning loop\n",
    "for ps_it in range(ps_iters):\n",
    "    if ps_it > 0 or exp.n_samples() == 0:\n",
    "        # apply policy\n",
    "        ret = utils.apply_controller(\n",
    "            env,\n",
    "            pol,\n",
    "            control_H,\n",
    "            callback=lambda *args, **kwargs: env.render(),\n",
    "            realtime=False, stop_when_done=False)\n",
    "        params_ = [p.clone() for p in list(pol.parameters())]\n",
    "        exp.append_episode(*ret, policy_params=params_)\n",
    "        exp.save(results_filename)\n",
    "\n",
    "    # train dynamics\n",
    "    X, Y = exp.get_dynmodel_dataset(deltas=True, return_costs=learn_reward)\n",
    "    dyn.set_dataset(X.to(dyn.X.device).float(), Y.to(dyn.X.device).float())\n",
    "    utils.train_regressor(dyn,\n",
    "                          dyn_opt_iters,\n",
    "                          N_particles,\n",
    "                          True,\n",
    "                          opt1,\n",
    "                          log_likelihood=dyn.output_density.log_prob,\n",
    "                          summary_writer=writer,\n",
    "                          summary_scope='model_learning/episode_%d' %\n",
    "                          ps_it)\n",
    "\n",
    "    # sample initial states for policy optimization\n",
    "    x0 = exp.sample_states(N_particles,\n",
    "                           timestep=0).to(dyn.X.device).float()\n",
    "    x0 = x0 + 1e-1 * torch.randn_like(x0)\n",
    "    x0 = x0.detach()\n",
    "\n",
    "    utils.plot_rollout(x0, dyn, pol, control_H)\n",
    "\n",
    "    # train policy\n",
    "    def on_iteration(i, loss, states, actions, rewards, discount):\n",
    "        writer.add_scalar('mc_pilco/episode_%d/training loss' % ps_it,\n",
    "                          loss, i)\n",
    "        if i % 100 == 0:\n",
    "            '''\n",
    "            states = states.transpose(0, 1).cpu().detach().numpy()\n",
    "            actions = actions.transpose(0, 1).cpu().detach().numpy()\n",
    "            rewards = rewards.transpose(0, 1).cpu().detach().numpy()\n",
    "            utils.plot_trajectories(states,\n",
    "                                    actions,\n",
    "                                    rewards,\n",
    "                                    plot_samples=True)\n",
    "            '''\n",
    "            writer.flush()\n",
    "\n",
    "    print(\"Policy search iteration %d\" % (ps_it + 1))\n",
    "    algorithms.mc_pilco(x0,\n",
    "                        dyn,\n",
    "                        pol,\n",
    "                        pred_H,\n",
    "                        opt2,\n",
    "                        exp,\n",
    "                        pol_opt_iters,\n",
    "                        pegasus=True,\n",
    "                        mm_states=True,\n",
    "                        mm_rewards=True,\n",
    "                        maximize=True,\n",
    "                        clip_grad=1.0,\n",
    "                        on_iteration=on_iteration,\n",
    "                        step_idx_to_sample=0,\n",
    "                        init_state_noise=1e-1 * x0.std(0))\n",
    "    utils.plot_rollout(x0, dyn, pol, control_H)\n",
    "    writer.add_scalar('robot/evaluation_loss',\n",
    "                      torch.tensor(ret[2]).sum(), ps_it + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
